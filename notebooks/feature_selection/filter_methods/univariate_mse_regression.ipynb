{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods - Univariate mse\n",
    "\n",
    "This procedure works as follows:\n",
    "\n",
    "- First, it builds one decision tree per feature, to predict the target\n",
    "- Second, it makes predictions using the decision tree and the mentioned feature\n",
    "- Third, it ranks the features according to the machine learning metric (mse)\n",
    "- It selects the highest ranked features"
   ]
  },
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### load dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# load dataset and features from previus method\n",
    "features = np.load('../features/featuresFromUnivariateClassif.npy').tolist()\n",
    "data = pd.read_pickle('../../data/features/features.pkl').loc[:,features].sample(frac=0.35).fillna(-9999)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "data.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train - test"
   ]
  },
  {
   "source": [
    "# In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n",
    "\n",
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### calculate metric for each variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# loop to build a tree, make predictions and get the mse\n",
    "# for each feature of the train set\n",
    "mse_values = []\n",
    "for feature in X_train.columns:\n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n",
    "    y_scored = clf.predict(X_test[feature].fillna(0).to_frame())\n",
    "    mse_values.append(mean_squared_error(y_test, y_scored))"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "mse_values = pd.Series(mse_values)\n",
    "mse_values.index = X_train.columns\n",
    "\n",
    "# Remember that for regression, the smaller the mse, the better the model performance is. So in this case, we need to select from the right to the left.\n",
    "mse_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### save features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the mse, you have to set up the cut-off value. The value will depend on how many features you would like to end up with.\n",
    "\n",
    "features_to_keep = mse_values[mse_values < CUTOFF].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../features/featuresFromUnivariateMSERegression.npy',features_to_keep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}