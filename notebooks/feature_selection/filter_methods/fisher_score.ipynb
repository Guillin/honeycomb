{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods - Fisher Score\n",
    "\n",
    "Compute chi-squared stats between each non-negative feature and class. \n",
    "\n",
    "- This score should be used to evaluate categorical variables in a classification task.\n",
    "\n",
    "It compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories. I explained this in more detail the introductory lecture of this section.\n"
   ]
  },
  {
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import preprocessing"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# load dataset and features from previus method\n",
    "features = np.load('../features/featuresFromMIClassif.npy').tolist()\n",
    "data = pd.read_pickle('../../data/features/features.pkl').loc[:,features]\n",
    "\n",
    "\n",
    "data.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### encode categorical variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# select categorical variables in the dataset and encode them into numbers\n",
    "catvars = ['var1', 'var2', 'var3',..., 'varn']\n",
    "\n",
    "# for each categorical variable do this\n",
    "for catvar in catvars:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    data.loc[:,catvar] = le.fit_transform(data.loc[:,catvar])\n"
   ],
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### split train - test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.\n",
    "\n",
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[catvars],\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### calculate chi2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# calculate the chi2 p_value between each of the variables\n",
    "# and the target\n",
    "# it returns 2 arrays, one contains the F-Scores which are then \n",
    "# evaluated against the chi2 distribution to obtain the pvalue\n",
    "# the pvalues are in the second array, see below\n",
    "\n",
    "f_score = chi2(X_train.fillna(-9999), y_train)\n",
    "f_score"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "\n",
    "pvalues = pd.Series(f_score[1])\n",
    "pvalues.index = X_train.columns\n",
    "pvalues.sort_values(ascending=False)"
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, that contrarily to MI, where we were interested in the higher MI values, for Fisher score, the smaller the p_value, the more significant the feature is to predict the target.\n",
    "\n",
    "**Note**\n",
    "One thing to keep in mind when using Fisher score or univariate selection methods, is that in very big datasets, most of the features will show a small p_value, and therefore look like they are highly predictive. This is in fact an effect of the sample size. So care should be taken when selecting features using these procedures. An ultra tiny p_value does not highlight an ultra-important feature, it rather indicates that the dataset contains too many samples. "
   ]
  },
  {
   "source": [
    "### save features"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many var would you like to keep from the previous fisher analysis \n",
    "NCATVAR = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = pvalues.sort_values(ascending=True).index.tolist()[:NCATVAR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../features/featuresFromFisherScore.npy',features_to_keep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ds': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1f33512659712c6a6ac90d8f6217146f941852120cd4340b6d127417f394cbe5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}